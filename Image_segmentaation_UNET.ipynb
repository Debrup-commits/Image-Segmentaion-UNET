{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_segmentaation_UNET.ipynb",
      "provenance": [],
      "mount_file_id": "1Qsdl4b-NZPguxZ_faFz3jBZmV8jfKnYe",
      "authorship_tag": "ABX9TyOz1j3w+RogUFMw3xrO8yH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debrup-commits/Image-Segmentaion-UNET/blob/main/Image_segmentaation_UNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "oIiFn9V1PluS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3YaFMbD-j0j",
        "outputId": "dbbac81b-4cc0-43cb-a136-40cb5f1c16c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/NN\\ Tasks/Semantic\\ Segmentation\\ UNET/train.zip /content/\n",
        "!cp /content/drive/MyDrive/NN\\ Tasks/Semantic\\ Segmentation\\ UNET/train_masks.zip /content/\n"
      ],
      "metadata": {
        "id": "NTepWB4aPo_T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/train.zip\n",
        "!unzip -q /content/train_masks.zip"
      ],
      "metadata": {
        "id": "WaMN5RXdOCmH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check if any missing masks as training mask and image folders have unequal size\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path/directory\n",
        "folder_dir = \"/content/train\"\n",
        "count=0\n",
        "\n",
        "# iterating through train\n",
        "for images in os.listdir(folder_dir):\n",
        "\n",
        "    # getting mask path for each train image\n",
        "    path= os.path.join('/content/train_masks/', images[:-4]+'_mask.gif')\n",
        "    \n",
        "    if os.path.isfile(path)==False:\n",
        "      count+=1\n",
        "      print(\"{} Mask(s) absent\".format(count))\n",
        "\n"
      ],
      "metadata": {
        "id": "59-CCsUOQ5Y9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generating val and val_masks folders\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "val_path='/content/val'\n",
        "val_mask_path='/content/val_masks'\n",
        "\n",
        "if os.path.isdir(val_path)==False:\n",
        "  os.mkdir(val_path)\n",
        "  os.mkdir(val_mask_path)\n",
        "\n",
        "train = '/content/train'\n",
        "train_mask = '/content/train_masks'\n",
        "\n",
        "i=1\n",
        "for images in os.listdir(train):\n",
        "  if i<49:\n",
        "    image_path = os.path.join('/content/train/', images)\n",
        "    image_mask_path = os.path.join('/content/train_masks/', images[:-4]+'_mask.gif')\n",
        "\n",
        "    target_image_path = os.path.join('/content/val/', images)\n",
        "    target_mask_path = os.path.join('/content/val_masks/', images[:-4]+'_mask.gif')\n",
        "\n",
        "    shutil.copyfile(image_path, target_image_path)\n",
        "    shutil.copyfile(image_mask_path, target_mask_path)\n",
        "\n",
        "  else:\n",
        "    break\n",
        "  \n",
        "  i+=1"
      ],
      "metadata": {
        "id": "76nsGdiCwvsN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "gfIp-9bLOEWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class CarvanaDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "    self.img_dir=img_dir\n",
        "    self.mask_dir=mask_dir\n",
        "    self.transform=transform\n",
        "    self.images=os.listdir(img_dir)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img_path=os.path.join(self.img_dir, self.images[index])\n",
        "      mask_path=os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
        "\n",
        "      image=np.array(Image.open(img_path).convert(\"RGB\")) #img converted to RGB np array\n",
        "      mask=np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32) #mask converted to BnW np array\n",
        "\n",
        "      #normalizing the mask for smaller calculations while training\n",
        "      mask[mask==255.0]=1.0\n",
        "\n",
        "      if self.transform is not None:\n",
        "        transformations=self.tansform(image=image, mask=mask)\n",
        "        image=transformations[\"image\"]\n",
        "        mask=transformations[\"mask\"]\n",
        "\n",
        "      return image, mask"
      ],
      "metadata": {
        "id": "40AdF8aSQ4mD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Our** **Model**"
      ],
      "metadata": {
        "id": "cS1Dm9lRZDlr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq_1EwALY8Df",
        "outputId": "f936e4df-86b9-4e8c-8899-813c367e8385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 160, 160]) torch.Size([3, 1, 160, 160])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as f\n",
        "\n",
        "# making the UNET structure\n",
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "    super(UNET, self).__init__()\n",
        "    self.downs=nn.ModuleList()\n",
        "    self.ups=nn.ModuleList()\n",
        "    self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Down part of UNET\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels=feature\n",
        "\n",
        "    for feature in reversed(features):\n",
        "      self.ups.append(nn.ConvTranspose2d(2*feature, feature, kernel_size=2, stride=2))\n",
        "      self.ups.append(DoubleConv(2*feature, feature))\n",
        "\n",
        "    self.bottle_neck=DoubleConv(features[-1], 2*features[-1])\n",
        "\n",
        "    self.final_conv=nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections=[]\n",
        "\n",
        "    for down in self.downs:\n",
        "      x=down(x)\n",
        "      skip_connections.append(x)\n",
        "      x=self.pool(x)\n",
        "\n",
        "    x=self.bottle_neck(x)\n",
        "    skip_connections=skip_connections[::-1]\n",
        "\n",
        "    for idx in range(0, len(self.ups), 2):\n",
        "      x=self.ups[idx](x)\n",
        "      skip_connection=skip_connections[idx//2]\n",
        "\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = f.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "      concatenated_output=torch.cat((skip_connection, x), dim=1)\n",
        "      x=self.ups[idx+1](concatenated_output)\n",
        "\n",
        "    x=self.final_conv(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def test():\n",
        "  x=torch.randn((3, 1, 160, 160))\n",
        "  model=UNET(in_channels=1, out_channels=1)\n",
        "  preds=model(x)\n",
        "\n",
        "  print(x.shape, preds.shape)\n",
        "\n",
        "  assert preds.shape==x.shape\n",
        "\n",
        "if __name__=='__main__':\n",
        "  test()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper functions**"
      ],
      "metadata": {
        "id": "iEZRo-gxXCO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"Saving CheckPoint!\")\n",
        "  torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "  print(\"Loading CheckPoint!\")\n",
        "  model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_mask_dir,\n",
        "    val_dir,\n",
        "    val_mask_dir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True):\n",
        "  \n",
        "  train_dataset=CarvanaDataset(train_dir, train_mask_dir, train_transform)\n",
        "  train_loader=DataLoader(train_dataset, batch_size, num_workers, pin_memory, shuffle=True)\n",
        "\n",
        "  val_dataset=CarvanaDataset(val_dir, val_mask_dir, val_transform)\n",
        "  val_loader=DataLoader(val_dataset, batch_size, num_workers, pin_memory, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "  num_correct=0\n",
        "  num_pixels=0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x=x.to(device)\n",
        "      y=y.to(device)\n",
        "\n",
        "      preds=torch.sigmoid(model(x))\n",
        "      preds=(preds>0.5).float()\n",
        "\n",
        "      num_correct+=(preds==y).sum()\n",
        "      num_pixels+=torch.numel(preds)\n",
        "\n",
        "  print(\"Accuracy: {}:.2f\".format(num_correct/num_pixels*100))\n",
        "  model.train()\n",
        "\n",
        "def save_predictions_as_imgs(loader, model, folder='/content/segmented_masks', device=\"cuda\"):\n",
        "  model.eval()\n",
        "  for idx, (x, y) in enumerate(loader):\n",
        "    x=x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      preds=torch.sigmoid(model(x))\n",
        "      preds=(preds>0.5).float()\n",
        "    \n",
        "    torchvision.utils.save_image(\n",
        "        preds, f\"{folder}/pred_{idx}.png\"\n",
        "    )\n",
        "    torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
        "\n",
        "  model.train()\n"
      ],
      "metadata": {
        "id": "7Z6-pK-PXI6s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "t0BxqJU78Mop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "FyRYyTYwg9sR",
        "outputId": "00deb14f-8a49-47e9-d8b8-318b8e847013"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.2.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=0b25e1c1d0e80790401a2f6e8505928a5f75374415773d2de8cda785890e0010\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "albumentations",
                  "imgaug"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#HyperParameters\n",
        "learning_rate=1e-4\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size=16\n",
        "num_epochs=100\n",
        "num_workers=2\n",
        "image_height=160\n",
        "image_width=240\n",
        "pin_memory=True\n",
        "load_model=False\n",
        "\n",
        "train_img_path='/content/train'\n",
        "train_mask_path='/content/train_masks'\n",
        "val_img_path='/content/val'\n",
        "val_mask_path='/content/val_masks'\n",
        "\n",
        "def train(loader, model, optimizer, loss_fn, scaler):\n",
        "  loop=tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "    data=data.to(device)\n",
        "    targets = targets.float().unsqueeze(1).to(device=device)\n",
        "\n",
        "    # forward\n",
        "    with torch.cuda.amp.autocast(): \n",
        "      predictions=model(data)\n",
        "      loss=loss_fn(predictions, targets)\n",
        "\n",
        "    # backward\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  train_transforms=A.compose([\n",
        "                              A.Resize(height=image_height, width=image_width),\n",
        "                              A.Rotate(limit=35, p=1.0),\n",
        "                              A.HorizontalFlip(p=0.5),\n",
        "                              A.VerticalFlip(p=0.1),\n",
        "                              A.Normalize(\n",
        "                                  mean=[0.0, 0.0, 0.0],\n",
        "                                  std=[1.0, 1.0, 1.0],\n",
        "                                  max_pixel_value=255.0,\n",
        "                              ),\n",
        "                              ToTensorV2()\n",
        "  ])\n",
        "\n",
        "  val_transforms = A.compose([\n",
        "                              A.Resize(height=image_height, width=image_width),\n",
        "                              A.Normalize(\n",
        "                                  mean=[0.0, 0.0, 0.0],\n",
        "                                  std=[1.0, 1.0, 1.0],\n",
        "                                  max_pixel_value=255.0,\n",
        "                              ),\n",
        "                              ToTensorV2()                            \n",
        "  ])\n",
        "\n",
        "  model = UNET(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "  # Binary cross entropy with logits loss\n",
        "  loss_fn = nn.BCEWithLogitsLoss() \n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train_loader, val_loader = get_loaders(\n",
        "      train_img_path,\n",
        "      train_mask_path,\n",
        "      val_img_path,\n",
        "      val_mask_path,\n",
        "      batch_size,\n",
        "      train_transforms,\n",
        "      val_transforms,\n",
        "      num_workers,\n",
        "      pin_memory\n",
        "  )\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    train(train_loader, model, optimizer, loss_fn, scaler)\n",
        "    checkpoint = {\n",
        "        \"state_dict\" : model.state_dict(),\n",
        "        \"optimizer\" : optimizer.state_dict()\n",
        "    }\n",
        "    save_checkpoint(checkpoint)\n",
        "\n",
        "    # check accuracy\n",
        "    check_accuracy(val_loader, model, device=device)\n",
        "\n",
        "    # save some predictions\n",
        "    saved_img_path='/content/segmented_masks'\n",
        "    if os.path.isdir(saved_img_path)==False:\n",
        "      os.mkdir(saved_img_path)\n",
        "\n",
        "    save_predictions_as_imgs(val_loader, folder=saved_img_path, device=device)\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "id": "0qHx-pQs8GPm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2f4590f0-d4b4-45d7-ab1c-315854187a08"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b5af58e526a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensorV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToTensorV2' from 'albumentations.pytorch' (/usr/local/lib/python3.7/dist-packages/albumentations/pytorch/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}